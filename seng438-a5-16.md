**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group \#:      | 16                        |
| -------------- | ------------------------- |
| Student Names: | Dominic Choi              |
|                | Angelo Jerome T. Reynante |
|                | Nathan Ante               |
|                | Karam Baroud              |

# Introduction
This lab introduces the usage of external tools to assess software reliability. We used 2 methods to assess software reliability: Reliability Growth Testing and Reliability Demonstration Chart (RDC).

Our group is meant to select a failure dataset from the numerous ones provided and use the 2 assessment methods to determine software reliability and whether the software meets standards or not. 

In Part 1, we used SRTAT and the models provided to assess reliability using Reliability Growth Testing.
In Part 2, we used SRTAT again to create RDCs and determine whether the software being assessed should be accepted, rejected, or continue being tested.

Our initial knowledge of Software Reliability Assessment came from content covered in lectures. The lectures explained why and how to use tools like SRTAT to assess software reliability. When using RDCs, we can edit values like Customer Risk, Supplier Risk, and Discrimination Ratio to create the boundaries between the reject, continue, and accept regions. With Reliability Growth Testing, we can create graphs using failure-count or time-between-failure data to visualize software failures and reliability. These graphs help easily determine whether a software product is becoming more reliable over time.

# Assessment Using Reliability Growth Testing 
We used SRTAT to assess with RGT and we used Failure Report 8<br/>

<figure>
<figcaption>We can see that the reliability increases over time</figcaption>
<img src="./images/RGT_default.png" title="test">
</figure>


# Assessment Using Reliability Demonstration Chart 
We used SRTAT to assess with RDC and we used Failure Report 8<br/>

When using the RDC, playing with different values can changes the results drastically. 
- Increasing the Customer Risk and/or Developer Risk causes the continue region to shrink which results in less testing being needed to determine if the software should be rejected or accepted. 
- Decreasing Customer Risk and/or Developer Risk widens the continue region and results in the system requiring more testing.
- Increasing the Discrimination Ratio causes the continue region to shrink, which results in the system requiring less testing to accept or reject it.
- Decreasing the Discrimination Ratio widens the continue region and results in requiring more testing.

<!-- - here is the time between failures dataset used for RDC assessment -->

We used **Failure Report 8.txt**

<figure>
<figcaption>The Failure Intensity(Failures Per Second) that resulted in the minimum MTTFmin that is still acceptable was 1.45 Failures/second.</figcaption>
<img src="./images/RDC_minimum_intensity.png" title="test">
</figure>

<figure>
<figcaption>Doubling this value causes failure data to highly exceed the acceptable threshold.</figcaption>
<img src="./images/RDC_double_intensity.png">
</figure>

<figure>
<figcaption>
Halving this value causes the failure data to fall quite short of this threshold which means the SUT still needs more testing.
</figcaption>
<img src="./images/RDC_half_intensity.png">
</figure>


# Comparison of Results



# Discussion on Similarity and Differences of the Two Techniques


# How the team work/effort was divided and managed
We worked on it together in-person

# Difficulties encountered, challenges overcome, and lessons learned
We had issues on importing the data in one of the example files in the sample input folder. Now, we've learned how to import the data on the STRAT application by just copying the data from the Failure Report document and paste the choosen column in the text file. Then run it in STRAT

# Comments/feedback on the lab itself
Overall, this lab's instructions were a bit confusing since we did not know we had to copy data to text files. We also had trouble using SRTAT with Mac and there was no troubleshooting material to help. This lab was nice and short though, and was a good way to end the semester. Thank!
